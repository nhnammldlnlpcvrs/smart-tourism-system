import os
import asyncio
from dotenv import load_dotenv
import google.generativeai as genai

from app.service.weather.weather_module import get_current_weather
from app.service.map.map_module import get_nearby_places, get_distance
from app.service.tourism.tourism_module import get_category_tree_by_province
from app.service.hotel.hotel_module import get_hotels_by_province_and_place_id
from app.service.foods.food_module import get_foods_by_province_and_tag

# C·∫§U H√åNH V√Ä KH·ªûI T·∫†O MODEL
# T·∫Øt logging GRPC (ƒê·ªÉ terminal s·∫°ch s·∫Ω h∆°n)
os.environ["GRPC_VERBOSITY"] = "NONE"
os.environ["GRPC_TRACE"] = ""

load_dotenv()
genai.configure(api_key=os.getenv("GEMINI_API_KEY"))

# T·ªïng h·ª£p t·∫•t c·∫£ c√°c tools m√† Chatbot c√≥ th·ªÉ s·ª≠ d·ª•ng
chat_tools = [
    get_current_weather,
    get_nearby_places,
    get_distance,
    get_category_tree_by_province, # ƒê·ªÉ l·∫•y c·∫•u tr√∫c du l·ªãch
    get_hotels_by_province_and_place_id, # G·ª£i √Ω kh√°ch s·∫°n
    get_foods_by_province_and_tag    # G·ª£i √Ω m√≥n ƒÉn
]

chat_model = genai.GenerativeModel(
    model_name="gemini-2.5-flash", # D√πng model 2.5 flash cho t·ªëc ƒë·ªô
    tools=chat_tools,
    system_instruction="B·∫°n l√† Tr·ª£ l√Ω Du l·ªãch Vi·ªát Nam, chuy√™n cung c·∫•p th√¥ng tin th·ªùi ti·∫øt, ƒë·ªãa ƒëi·ªÉm, m√≥n ƒÉn v√† ch·ªó ·ªü. Tr·∫£ l·ªùi ng·∫Øn g·ªçn, th√¢n thi·ªán v√† s·ª≠ d·ª•ng c√°c c√¥ng c·ª• khi c·∫ßn thi·∫øt."
)
# Kh·ªüi t·∫°o session chat ƒë·ªÉ gi·ªØ l·ªãch s·ª≠ h·ªôi tho·∫°i
chat_session = chat_model.start_chat(history=[])

# MODEL CHUY√äN D·ª§NG: WRITER (Vi·∫øt l·ªùi b√¨nh)
writer_model = genai.GenerativeModel(
    model_name="gemini-2.5-flash",
    generation_config={"temperature": 0.8, "max_output_tokens": 200},
    system_instruction="B·∫°n l√† m·ªôt h∆∞·ªõng d·∫´n vi√™n du l·ªãch vui t√≠nh, am hi·ªÉu vƒÉn h√≥a Vi·ªát Nam."
)

# CHUY√äN T·∫†O L·ªúI B√åNH LU·∫¨N CHO APP (S·ª≠ d·ª•ng writer_model)
async def generate_smart_comment(city: str, service_type: str) -> str:
    """
    Sinh ra m·ªôt c√¢u b√¨nh lu·∫≠n ng·∫Øn g·ªçn, th√∫ v·ªã d·ª±a tr√™n ƒë·ªãa ƒëi·ªÉm v√† d·ªãch v·ª• ng∆∞·ªùi d√πng ƒëang xem.
    """
    prompt = ""
    
    if service_type == "hotel":
        prompt = f"Ng∆∞·ªùi d√πng ƒëang t√¨m kh√°ch s·∫°n t·∫°i {city}. H√£y vi·∫øt m·ªôt c√¢u (1-2 c√¢u) khen ng·ª£i {city} v√† m·ªùi h·ªç xem danh s√°ch kh√°ch s·∫°n b√™n d∆∞·ªõi. V√≠ d·ª•: 'Woa, {city} m√πa n√†y ƒë·∫πp l·∫Øm! D∆∞·ªõi ƒë√¢y l√† m·∫•y kh√°ch s·∫°n view x·ªãn m√¨nh t√¨m ƒë∆∞·ª£c n√® üëá'"
    elif service_type == "food":
        prompt = f"Ng∆∞·ªùi d√πng ƒëang t√¨m m√≥n ƒÉn t·∫°i {city}. H√£y vi·∫øt m·ªôt c√¢u (1-2 c√¢u) nh·∫Øc ƒë·∫øn m·ªôt ƒë·∫∑c s·∫£n n·ªïi ti·∫øng c·ªßa {city} v√† m·ªùi h·ªç xem danh s√°ch. V√≠ d·ª•: 'ƒê·∫øn {city} m√† kh√¥ng ƒÉn [ƒë·∫∑c s·∫£n] l√† ph√≠ l·∫Øm nha! Xem ngay list qu√°n ngon n√†y üëá'"
    elif service_type == "place":
        prompt = f"Ng∆∞·ªùi d√πng ƒëang xem ƒë·ªãa ƒëi·ªÉm tham quan t·∫°i {city}. H√£y vi·∫øt m·ªôt c√¢u h√†o h·ª©ng r·ªß h·ªç x√°ch ba l√¥ l√™n v√† ƒëi."
    else:
        prompt = f"Ch√†o m·ª´ng b·∫°n ƒë·∫øn v·ªõi {city}. D∆∞·ªõi ƒë√¢y l√† th√¥ng tin b·∫°n c·∫ßn."

    try:
        # G·ªçi model sinh text thu·∫ßn t√∫y -> Nhanh & R·∫ª
        response = await writer_model.generate_content_async(prompt)
        return response.text.strip()
    except Exception:
        return f"Ch√†o b·∫°n! D∆∞·ªõi ƒë√¢y l√† danh s√°ch {service_type} t·∫°i {city} m√¨nh t√¨m ƒë∆∞·ª£c nha! üëá"

# H√ÄM CH√çNH: X·ª¨ L√ù CHATBOT T·ª∞ DO (S·ª≠ d·ª•ng chat_session)
async def ask_gemini(user_prompt: str):
    """
    X·ª≠ l√Ω y√™u c·∫ßu c·ªßa ng∆∞·ªùi d√πng, g·ªçi c√°c c√¥ng c·ª• (tools) n·∫øu c·∫ßn thi·∫øt.
    """
    response = await chat_session.send_message_async(user_prompt)

    if response.function_calls:
        print(f" DEBUG: Model quy·∫øt ƒë·ªãnh g·ªçi {len(response.function_calls)} tool.")
        
        # T·∫°o list c√°c task (c√¥ng vi·ªác) b·∫•t ƒë·ªìng b·ªô ƒë·ªÉ g·ªçi c√°c tool
        tool_results = []
        for call in response.function_calls:
            # L·∫•y h√†m c·∫ßn g·ªçi t·ª´ global scope
            tool_func = globals().get(call.name)
            if tool_func:
                # Th·ª±c hi·ªán g·ªçi h√†m v·ªõi c√°c ƒë·ªëi s·ªë m√† model cung c·∫•p
                # D√πng asyncio.to_thread n·∫øu h√†m l√† blocking (nh∆∞ c√°c h√†m DB/requests kh√¥ng ph·∫£i async)
                # Ho·∫∑c g·ªçi tr·ª±c ti·∫øp n·∫øu h√†m l√† async (nh∆∞ httpx/asyncpg)
                if asyncio.iscoroutinefunction(tool_func):
                    result = await tool_func(**dict(call.args))
                else:
                    result = await asyncio.to_thread(tool_func, **dict(call.args))
                
                tool_results.append(
                    genai.types.Part.from_function_response(name=call.name, response=result)
                )
            else:
                 print(f"Tool {call.name} not found.")
        
        # G·ª≠i k·∫øt qu·∫£ c·ªßa tool tr·ªü l·∫°i cho Model ƒë·ªÉ n√≥ t·ªïng h·ª£p c√¢u tr·∫£ l·ªùi
        response = await chat_session.send_message_async(tool_results)

    return response.text.strip()