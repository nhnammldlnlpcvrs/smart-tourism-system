{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e6bb37f5-c334-41c2-b847-fb7c76eb058b",
   "metadata": {},
   "source": [
    "# RAG Pipeline + PostgreSQL + SQLAlchemy\n",
    "\n",
    "Test RAG \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ab4df62-d45c-4c37-9e9e-c2c143a479d7",
   "metadata": {},
   "source": [
    "## Connect PostgreSQL through SQLAlchemy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "82ae5c65-8efc-49af-a164-10695d98f012",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "success\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "from sqlalchemy import create_engine, MetaData, Table, Column, Integer, JSON, select\n",
    "from sqlalchemy.orm import sessionmaker\n",
    "from sqlalchemy.orm import sessionmaker\n",
    "\n",
    "from sqlalchemy import create_engine, MetaData, Table, Column, Integer, JSON\n",
    "from sqlalchemy.orm import sessionmaker\n",
    "\n",
    "DATABASE_URL = \"postgresql+psycopg2://postgres:abc123@/tourismdb?host=/tmp\"\n",
    "\n",
    "engine = create_engine(DATABASE_URL, echo=True)\n",
    "\n",
    "SessionLocal = sessionmaker(bind=engine)\n",
    "session = SessionLocal()\n",
    "\n",
    "metadata = MetaData()\n",
    "print(\"success\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35fa03fb-8713-4e5b-8474-60c8cad2ac44",
   "metadata": {},
   "source": [
    "## Define rag table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a94a7043-a99d-4568-8587-161312bd73dc",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-12-02 23:02:01,820 INFO sqlalchemy.engine.Engine select pg_catalog.version()\n",
      "2025-12-02 23:02:01,821 INFO sqlalchemy.engine.Engine [raw sql] {}\n",
      "2025-12-02 23:02:01,825 INFO sqlalchemy.engine.Engine select current_schema()\n",
      "2025-12-02 23:02:01,825 INFO sqlalchemy.engine.Engine [raw sql] {}\n",
      "2025-12-02 23:02:01,827 INFO sqlalchemy.engine.Engine show standard_conforming_strings\n",
      "2025-12-02 23:02:01,827 INFO sqlalchemy.engine.Engine [raw sql] {}\n",
      "2025-12-02 23:02:01,834 INFO sqlalchemy.engine.Engine BEGIN (implicit)\n",
      "2025-12-02 23:02:01,843 INFO sqlalchemy.engine.Engine SELECT pg_catalog.pg_class.relname \n",
      "FROM pg_catalog.pg_class JOIN pg_catalog.pg_namespace ON pg_catalog.pg_namespace.oid = pg_catalog.pg_class.relnamespace \n",
      "WHERE pg_catalog.pg_class.relname = %(table_name)s AND pg_catalog.pg_class.relkind = ANY (ARRAY[%(param_1)s, %(param_2)s, %(param_3)s, %(param_4)s, %(param_5)s]) AND pg_catalog.pg_table_is_visible(pg_catalog.pg_class.oid) AND pg_catalog.pg_namespace.nspname != %(nspname_1)s\n",
      "2025-12-02 23:02:01,843 INFO sqlalchemy.engine.Engine [generated in 0.00057s] {'table_name': 'rag_documents', 'param_1': 'r', 'param_2': 'p', 'param_3': 'f', 'param_4': 'v', 'param_5': 'm', 'nspname_1': 'pg_catalog'}\n",
      "2025-12-02 23:02:01,848 INFO sqlalchemy.engine.Engine COMMIT\n",
      "Table 'rag_documents' created successfully!\n"
     ]
    }
   ],
   "source": [
    "from sqlalchemy import JSON\n",
    "\n",
    "rag_table = Table(\n",
    "    \"rag_documents\",\n",
    "    metadata,\n",
    "    Column(\"id\", Integer, primary_key=True, autoincrement=True),\n",
    "    Column(\"content\", JSON, nullable=False),\n",
    ")\n",
    "\n",
    "metadata.create_all(engine)\n",
    "print(\"Table 'rag_documents' created successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7b353ab-8a23-4fa1-a8d0-a607fa6141af",
   "metadata": {},
   "source": [
    "Testing sample"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52217023-fc51-471e-b3b4-45752216df40",
   "metadata": {},
   "source": [
    "## Loader: Input Data from SQL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "2fb73857-2960-4632-be8f-896f1426be17",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sqlalchemy import Table, MetaData\n",
    "from typing import List, Dict, Any\n",
    "\n",
    "def load_postgres_data_dynamic(engine, table_names: List[str]) -> List[Dict[str, Any]]:\n",
    "    metadata = MetaData()\n",
    "    results = []\n",
    "\n",
    "    with engine.begin() as conn:\n",
    "        for tbl_name in table_names:\n",
    "            table = Table(tbl_name, metadata, autoload_with=engine)\n",
    "\n",
    "            rows = conn.execute(table.select()).fetchall()\n",
    "            for r in rows:\n",
    "                row_dict = dict(r._mapping)\n",
    "                # ID có thể là id hoặc table_name:id\n",
    "                rid = row_dict.get(\"id\", None)\n",
    "\n",
    "                results.append({\n",
    "                    \"id\": f\"{tbl_name}:{rid}\",\n",
    "                    \"record\": row_dict  # giữ full JSON để embed\n",
    "                })\n",
    "\n",
    "    return results\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cb03e79-aa1e-42c9-a0d2-15d652e6b524",
   "metadata": {},
   "source": [
    "## Splitter : Split data into chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "eadd0b69-3132-45ba-9975-9bd0da078f80",
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_documents(data):\n",
    "    chunks = []\n",
    "    for item in data:\n",
    "        chunks.append({\n",
    "            \"id\": item[\"id\"],\n",
    "            \"record\": item[\"record\"]\n",
    "        })\n",
    "    return chunks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9871efbf-3809-4b04-9359-700872ccf44e",
   "metadata": {},
   "source": [
    "## Embedder : Data -> npArray"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "e9d57d98-be91-426f-b369-cdc706ce06a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "I0000 00:00:1764691321.872502 12285246 fork_posix.cc:71] Other threads are currently calling into gRPC, skipping fork() handlers\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pip in /opt/homebrew/lib/python3.14/site-packages (25.3)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0000 00:00:1764691323.032116 12285246 fork_posix.cc:71] Other threads are currently calling into gRPC, skipping fork() handlers\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: google-generativeai in /opt/homebrew/Cellar/jupyterlab/4.5.0_1/libexec/lib/python3.14/site-packages (0.8.5)\n",
      "Requirement already satisfied: numpy in /opt/homebrew/lib/python3.14/site-packages (2.3.4)\n",
      "Requirement already satisfied: google-ai-generativelanguage==0.6.15 in /opt/homebrew/Cellar/jupyterlab/4.5.0_1/libexec/lib/python3.14/site-packages (from google-generativeai) (0.6.15)\n",
      "Requirement already satisfied: google-api-core in /opt/homebrew/Cellar/jupyterlab/4.5.0_1/libexec/lib/python3.14/site-packages (from google-generativeai) (2.25.2)\n",
      "Requirement already satisfied: google-api-python-client in /opt/homebrew/Cellar/jupyterlab/4.5.0_1/libexec/lib/python3.14/site-packages (from google-generativeai) (2.187.0)\n",
      "Requirement already satisfied: google-auth>=2.15.0 in /opt/homebrew/Cellar/jupyterlab/4.5.0_1/libexec/lib/python3.14/site-packages (from google-generativeai) (2.43.0)\n",
      "Requirement already satisfied: protobuf in /opt/homebrew/Cellar/jupyterlab/4.5.0_1/libexec/lib/python3.14/site-packages (from google-generativeai) (5.29.5)\n",
      "Requirement already satisfied: pydantic in /opt/homebrew/Cellar/jupyterlab/4.5.0_1/libexec/lib/python3.14/site-packages (from google-generativeai) (2.12.5)\n",
      "Requirement already satisfied: tqdm in /opt/homebrew/Cellar/jupyterlab/4.5.0_1/libexec/lib/python3.14/site-packages (from google-generativeai) (4.67.1)\n",
      "Requirement already satisfied: typing-extensions in /opt/homebrew/Cellar/jupyterlab/4.5.0_1/libexec/lib/python3.14/site-packages (from google-generativeai) (4.15.0)\n",
      "Requirement already satisfied: proto-plus<2.0.0dev,>=1.22.3 in /opt/homebrew/Cellar/jupyterlab/4.5.0_1/libexec/lib/python3.14/site-packages (from google-ai-generativelanguage==0.6.15->google-generativeai) (1.26.1)\n",
      "Requirement already satisfied: googleapis-common-protos<2.0.0,>=1.56.2 in /opt/homebrew/Cellar/jupyterlab/4.5.0_1/libexec/lib/python3.14/site-packages (from google-api-core->google-generativeai) (1.72.0)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.18.0 in /opt/homebrew/Cellar/jupyterlab/4.5.0_1/libexec/lib/python3.14/site-packages (from google-api-core->google-generativeai) (2.32.5)\n",
      "Requirement already satisfied: grpcio<2.0.0,>=1.33.2 in /opt/homebrew/Cellar/jupyterlab/4.5.0_1/libexec/lib/python3.14/site-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.1->google-ai-generativelanguage==0.6.15->google-generativeai) (1.76.0)\n",
      "Requirement already satisfied: grpcio-status<2.0.0,>=1.33.2 in /opt/homebrew/Cellar/jupyterlab/4.5.0_1/libexec/lib/python3.14/site-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.1->google-ai-generativelanguage==0.6.15->google-generativeai) (1.71.2)\n",
      "Requirement already satisfied: cachetools<7.0,>=2.0.0 in /opt/homebrew/Cellar/jupyterlab/4.5.0_1/libexec/lib/python3.14/site-packages (from google-auth>=2.15.0->google-generativeai) (6.2.2)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in /opt/homebrew/Cellar/jupyterlab/4.5.0_1/libexec/lib/python3.14/site-packages (from google-auth>=2.15.0->google-generativeai) (0.4.2)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in /opt/homebrew/Cellar/jupyterlab/4.5.0_1/libexec/lib/python3.14/site-packages (from google-auth>=2.15.0->google-generativeai) (4.9.1)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /opt/homebrew/Cellar/jupyterlab/4.5.0_1/libexec/lib/python3.14/site-packages (from requests<3.0.0,>=2.18.0->google-api-core->google-generativeai) (3.4.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/homebrew/Cellar/jupyterlab/4.5.0_1/libexec/lib/python3.14/site-packages (from requests<3.0.0,>=2.18.0->google-api-core->google-generativeai) (3.11)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/homebrew/Cellar/jupyterlab/4.5.0_1/libexec/lib/python3.14/site-packages (from requests<3.0.0,>=2.18.0->google-api-core->google-generativeai) (2.5.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/homebrew/opt/certifi/lib/python3.14/site-packages (from requests<3.0.0,>=2.18.0->google-api-core->google-generativeai) (2025.11.12)\n",
      "Requirement already satisfied: pyasn1>=0.1.3 in /opt/homebrew/Cellar/jupyterlab/4.5.0_1/libexec/lib/python3.14/site-packages (from rsa<5,>=3.1.4->google-auth>=2.15.0->google-generativeai) (0.6.1)\n",
      "Requirement already satisfied: httplib2<1.0.0,>=0.19.0 in /opt/homebrew/Cellar/jupyterlab/4.5.0_1/libexec/lib/python3.14/site-packages (from google-api-python-client->google-generativeai) (0.31.0)\n",
      "Requirement already satisfied: google-auth-httplib2<1.0.0,>=0.2.0 in /opt/homebrew/Cellar/jupyterlab/4.5.0_1/libexec/lib/python3.14/site-packages (from google-api-python-client->google-generativeai) (0.2.1)\n",
      "Requirement already satisfied: uritemplate<5,>=3.0.1 in /opt/homebrew/Cellar/jupyterlab/4.5.0_1/libexec/lib/python3.14/site-packages (from google-api-python-client->google-generativeai) (4.2.0)\n",
      "Requirement already satisfied: pyparsing<4,>=3.0.4 in /opt/homebrew/Cellar/jupyterlab/4.5.0_1/libexec/lib/python3.14/site-packages (from httplib2<1.0.0,>=0.19.0->google-api-python-client->google-generativeai) (3.2.5)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in /opt/homebrew/Cellar/jupyterlab/4.5.0_1/libexec/lib/python3.14/site-packages (from pydantic->google-generativeai) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.41.5 in /opt/homebrew/Cellar/jupyterlab/4.5.0_1/libexec/lib/python3.14/site-packages (from pydantic->google-generativeai) (2.41.5)\n",
      "Requirement already satisfied: typing-inspection>=0.4.2 in /opt/homebrew/Cellar/jupyterlab/4.5.0_1/libexec/lib/python3.14/site-packages (from pydantic->google-generativeai) (0.4.2)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0000 00:00:1764691323.751579 12285246 fork_posix.cc:71] Other threads are currently calling into gRPC, skipping fork() handlers\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: faiss-cpu in /opt/homebrew/Cellar/jupyterlab/4.5.0_1/libexec/lib/python3.14/site-packages (1.13.0)\n",
      "Requirement already satisfied: numpy<3.0,>=1.25.0 in /opt/homebrew/lib/python3.14/site-packages (from faiss-cpu) (2.3.4)\n",
      "Requirement already satisfied: packaging in /opt/homebrew/Cellar/jupyterlab/4.5.0_1/libexec/lib/python3.14/site-packages (from faiss-cpu) (25.0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0000 00:00:1764691324.459372 12285246 fork_posix.cc:71] Other threads are currently calling into gRPC, skipping fork() handlers\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: python-dotenv in /opt/homebrew/Cellar/jupyterlab/4.5.0_1/libexec/lib/python3.14/site-packages (1.2.1)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0000 00:00:1764691325.147841 12285246 fork_posix.cc:71] Other threads are currently calling into gRPC, skipping fork() handlers\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: sentence-transformers in /opt/homebrew/Cellar/jupyterlab/4.5.0_1/libexec/lib/python3.14/site-packages (5.1.2)\n",
      "Requirement already satisfied: transformers<5.0.0,>=4.41.0 in /opt/homebrew/Cellar/jupyterlab/4.5.0_1/libexec/lib/python3.14/site-packages (from sentence-transformers) (4.57.3)\n",
      "Requirement already satisfied: tqdm in /opt/homebrew/Cellar/jupyterlab/4.5.0_1/libexec/lib/python3.14/site-packages (from sentence-transformers) (4.67.1)\n",
      "Requirement already satisfied: torch>=1.11.0 in /opt/homebrew/Cellar/jupyterlab/4.5.0_1/libexec/lib/python3.14/site-packages (from sentence-transformers) (2.9.1)\n",
      "Requirement already satisfied: scikit-learn in /opt/homebrew/Cellar/jupyterlab/4.5.0_1/libexec/lib/python3.14/site-packages (from sentence-transformers) (1.7.2)\n",
      "Requirement already satisfied: scipy in /opt/homebrew/Cellar/jupyterlab/4.5.0_1/libexec/lib/python3.14/site-packages (from sentence-transformers) (1.16.3)\n",
      "Requirement already satisfied: huggingface-hub>=0.20.0 in /opt/homebrew/Cellar/jupyterlab/4.5.0_1/libexec/lib/python3.14/site-packages (from sentence-transformers) (0.36.0)\n",
      "Requirement already satisfied: Pillow in /opt/homebrew/Cellar/jupyterlab/4.5.0_1/libexec/lib/python3.14/site-packages (from sentence-transformers) (12.0.0)\n",
      "Requirement already satisfied: typing_extensions>=4.5.0 in /opt/homebrew/Cellar/jupyterlab/4.5.0_1/libexec/lib/python3.14/site-packages (from sentence-transformers) (4.15.0)\n",
      "Requirement already satisfied: filelock in /opt/homebrew/Cellar/jupyterlab/4.5.0_1/libexec/lib/python3.14/site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (3.20.0)\n",
      "Requirement already satisfied: numpy>=1.17 in /opt/homebrew/lib/python3.14/site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (2.3.4)\n",
      "Requirement already satisfied: packaging>=20.0 in /opt/homebrew/Cellar/jupyterlab/4.5.0_1/libexec/lib/python3.14/site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (25.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /opt/homebrew/Cellar/jupyterlab/4.5.0_1/libexec/lib/python3.14/site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (6.0.3)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /opt/homebrew/Cellar/jupyterlab/4.5.0_1/libexec/lib/python3.14/site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (2025.11.3)\n",
      "Requirement already satisfied: requests in /opt/homebrew/Cellar/jupyterlab/4.5.0_1/libexec/lib/python3.14/site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (2.32.5)\n",
      "Requirement already satisfied: tokenizers<=0.23.0,>=0.22.0 in /opt/homebrew/Cellar/jupyterlab/4.5.0_1/libexec/lib/python3.14/site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (0.22.1)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in /opt/homebrew/Cellar/jupyterlab/4.5.0_1/libexec/lib/python3.14/site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (0.7.0)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /opt/homebrew/Cellar/jupyterlab/4.5.0_1/libexec/lib/python3.14/site-packages (from huggingface-hub>=0.20.0->sentence-transformers) (2025.10.0)\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /opt/homebrew/Cellar/jupyterlab/4.5.0_1/libexec/lib/python3.14/site-packages (from huggingface-hub>=0.20.0->sentence-transformers) (1.2.0)\n",
      "Requirement already satisfied: setuptools in /opt/homebrew/Cellar/jupyterlab/4.5.0_1/libexec/lib/python3.14/site-packages (from torch>=1.11.0->sentence-transformers) (80.9.0)\n",
      "Requirement already satisfied: sympy>=1.13.3 in /opt/homebrew/Cellar/jupyterlab/4.5.0_1/libexec/lib/python3.14/site-packages (from torch>=1.11.0->sentence-transformers) (1.14.0)\n",
      "Requirement already satisfied: networkx>=2.5.1 in /opt/homebrew/Cellar/jupyterlab/4.5.0_1/libexec/lib/python3.14/site-packages (from torch>=1.11.0->sentence-transformers) (3.6)\n",
      "Requirement already satisfied: jinja2 in /opt/homebrew/Cellar/jupyterlab/4.5.0_1/libexec/lib/python3.14/site-packages (from torch>=1.11.0->sentence-transformers) (3.1.6)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /opt/homebrew/Cellar/jupyterlab/4.5.0_1/libexec/lib/python3.14/site-packages (from sympy>=1.13.3->torch>=1.11.0->sentence-transformers) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /opt/homebrew/Cellar/jupyterlab/4.5.0_1/libexec/lib/python3.14/site-packages (from jinja2->torch>=1.11.0->sentence-transformers) (3.0.3)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /opt/homebrew/Cellar/jupyterlab/4.5.0_1/libexec/lib/python3.14/site-packages (from requests->transformers<5.0.0,>=4.41.0->sentence-transformers) (3.4.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/homebrew/Cellar/jupyterlab/4.5.0_1/libexec/lib/python3.14/site-packages (from requests->transformers<5.0.0,>=4.41.0->sentence-transformers) (3.11)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/homebrew/Cellar/jupyterlab/4.5.0_1/libexec/lib/python3.14/site-packages (from requests->transformers<5.0.0,>=4.41.0->sentence-transformers) (2.5.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/homebrew/opt/certifi/lib/python3.14/site-packages (from requests->transformers<5.0.0,>=4.41.0->sentence-transformers) (2025.11.12)\n",
      "Requirement already satisfied: joblib>=1.2.0 in /opt/homebrew/Cellar/jupyterlab/4.5.0_1/libexec/lib/python3.14/site-packages (from scikit-learn->sentence-transformers) (1.5.2)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in /opt/homebrew/Cellar/jupyterlab/4.5.0_1/libexec/lib/python3.14/site-packages (from scikit-learn->sentence-transformers) (3.6.0)\n"
     ]
    }
   ],
   "source": [
    "!\"/opt/homebrew/Cellar/jupyterlab/4.5.0_1/libexec/bin/python\" -m pip install --upgrade pip\n",
    "!\"/opt/homebrew/Cellar/jupyterlab/4.5.0_1/libexec/bin/python\" -m pip install google-generativeai numpy\n",
    "!\"/opt/homebrew/Cellar/jupyterlab/4.5.0_1/libexec/bin/python\" -m pip install faiss-cpu\n",
    "!\"/opt/homebrew/Cellar/jupyterlab/4.5.0_1/libexec/bin/python\" -m pip install python-dotenv\n",
    "!\"/opt/homebrew/Cellar/jupyterlab/4.5.0_1/libexec/bin/python\" -m pip install sentence-transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "b1efb3da-a944-4983-b4a7-5bf00e7ccaf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "import numpy as np\n",
    "\n",
    "# Load model 1 lần\n",
    "_e5_model = SentenceTransformer(\"intfloat/multilingual-e5-small\")\n",
    "\n",
    "def _record_to_text(record):\n",
    "    if isinstance(record, dict):\n",
    "        return \" \".join(f\"{k}: {v}\" for k, v in record.items() if v)\n",
    "    elif isinstance(record, str):\n",
    "        return record\n",
    "    else:\n",
    "        raise TypeError(f\"Unsupported record type: {type(record)}\")\n",
    "\n",
    "\n",
    "def get_embeddings(records):\n",
    "    \"\"\"\n",
    "    records: danh sách dạng [{\"record\": {...}}, ...]\n",
    "    return: numpy array (float32) emb\n",
    "    \"\"\"\n",
    "    texts = [_record_to_text(r[\"record\"]) for r in records]\n",
    "\n",
    "    embeddings = _e5_model.encode(\n",
    "        texts,\n",
    "        convert_to_numpy=True,\n",
    "        show_progress_bar=True\n",
    "    )\n",
    "\n",
    "    return embeddings.astype(\"float32\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "323bf427-deff-41ce-b9eb-411adf8f9670",
   "metadata": {},
   "source": [
    "## VectorStore : Store the embedded data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "f7ef72eb-52cd-41d1-a108-aed11b651e3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import faiss\n",
    "import numpy as np\n",
    "import os\n",
    "import pickle\n",
    "from typing import List, Dict, Optional, Any\n",
    "\n",
    "\"\"\"\n",
    "    VectorStore dùng FAISS IndexFlatL2 (mặc định). Lưu embeddings + records (metadata).\n",
    "    - vectors: np.ndarray shape (n, d)\n",
    "    - records: list[dict] tương ứng\n",
    "    - persist_path: nếu truyền sẽ ghi faiss.index và records.pkl\n",
    "\"\"\"\n",
    "class VectorStore:\n",
    "\n",
    "    def __init__(self, vectors: np.ndarray, records: List[Dict[str, Any]], persist_path: Optional[str] = None):\n",
    "        assert len(vectors) == len(records), \"vectors and records must have same length\"\n",
    "        self.records = records\n",
    "        self.d = vectors.shape[1]\n",
    "\n",
    "        self.index = faiss.IndexFlatL2(self.d)\n",
    "        self.index.add(vectors.astype(\"float32\"))\n",
    "\n",
    "        self.persist_path = persist_path\n",
    "        if persist_path:\n",
    "            os.makedirs(persist_path, exist_ok=True)\n",
    "            faiss.write_index(self.index, os.path.join(persist_path, \"faiss.index\"))\n",
    "            with open(os.path.join(persist_path, \"records.pkl\"), \"wb\") as f:\n",
    "                pickle.dump(self.records, f)\n",
    "\n",
    "\n",
    "    @classmethod\n",
    "    def load(cls, persist_path: str) -> \"VectorStore\":\n",
    "        \"\"\"\n",
    "        Tạo VectorStore từ persist_path (faiss.index + records.pkl).\n",
    "        \"\"\"\n",
    "        idx_path = os.path.join(persist_path, \"faiss.index\")\n",
    "        rec_path = os.path.join(persist_path, \"records.pkl\")\n",
    "        if not os.path.exists(idx_path) or not os.path.exists(rec_path):\n",
    "            raise FileNotFoundError(\"Persist files not found in persist_path\")\n",
    "\n",
    "        index = faiss.read_index(idx_path)\n",
    "        with open(rec_path, \"rb\") as f:\n",
    "            records = pickle.load(f)\n",
    "\n",
    "        d = index.d\n",
    "        placeholder = np.zeros((0, d), dtype=\"float32\")\n",
    "        inst = cls.__new__(cls)\n",
    "        inst.records = records\n",
    "        inst.d = d\n",
    "        inst.index = index\n",
    "        inst.persist_path = persist_path\n",
    "        return inst\n",
    "    \n",
    "\n",
    "    def search(self, query_vector: np.ndarray, top_k: int = 3) -> List[Dict[str, Any]]:\n",
    "        \"\"\"\n",
    "        Tìm top_k nearest records theo L2. \n",
    "        - query_vector shape: (1, d) hoặc (n, d) nhưng we assume (1, d) here.\n",
    "        Trả về list các record dict theo thứ tự gần nhất -> xa.\n",
    "        \"\"\"\n",
    "        if query_vector.ndim == 1:\n",
    "            query_vector = query_vector.reshape(1, -1)\n",
    "        distances, indices = self.index.search(query_vector.astype(\"float32\"), top_k)\n",
    "        inds = indices[0].tolist()\n",
    "        result = []\n",
    "        for i in inds:\n",
    "            if i < 0 or i >= len(self.records):\n",
    "                continue\n",
    "            result.append(self.records[i])\n",
    "        return result\n",
    "\n",
    "    def save(self) -> None:\n",
    "        \"\"\"\n",
    "        Explicit persist nếu muốn.\n",
    "        \"\"\"\n",
    "        if not self.persist_path:\n",
    "            raise RuntimeError(\"persist_path not configured\")\n",
    "        faiss.write_index(self.index, os.path.join(self.persist_path, \"faiss.index\"))\n",
    "        with open(os.path.join(self.persist_path, \"records.pkl\"), \"wb\") as f:\n",
    "            pickle.dump(self.records, f)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b597c369-d0be-4e6e-92fb-6d034e4ad2bd",
   "metadata": {},
   "source": [
    "## Retriever : Retrieve from embedded data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "14ca130a-4677-41d8-871d-d8d6a7f15c60",
   "metadata": {},
   "outputs": [],
   "source": [
    "def retrieve_relevant_docs(query: str, store: VectorStore, top_k: int = 3, embed_model: str = None) -> List[Dict[str, str]]:\n",
    "    # 1. Embed query đúng kiểu string\n",
    "    q_vec = get_embeddings([{\"record\": query}])  # chỉ dùng string, như khi build index\n",
    "\n",
    "    # 2. Lấy top_k records từ vector store\n",
    "    records = store.search(q_vec, top_k=top_k)  # giả sử trả về list[str] hoặc list[record]\n",
    "\n",
    "    # 3. Map về dict cho RAGPipelinePG\n",
    "    contexts = [{\"title\": f\"Doc {i}\", \"body\": r} for i, r in enumerate(records)]\n",
    "    return contexts\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36147337-77aa-4513-986f-6b8575c634d4",
   "metadata": {},
   "source": [
    "## Generator : Generate answer from user's queries and retrieved data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "ce95745e-4e5f-47e9-82e2-52903c50fd1a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Backend path: /Users/phungquochuy/smart-tourism-system/backend\n"
     ]
    }
   ],
   "source": [
    "from typing import List, Dict\n",
    "import sys\n",
    "import os\n",
    "\n",
    "# Lấy đường dẫn tuyệt đối tới backend\n",
    "backend_path = os.path.abspath(os.path.join(\"..\"))  # từ notebooks lên backend\n",
    "print(\"Backend path:\", backend_path)\n",
    "\n",
    "# Thêm vào sys.path nếu chưa có\n",
    "if backend_path not in sys.path:\n",
    "    sys.path.insert(0, backend_path)\n",
    "\n",
    "# Bây giờ import được\n",
    "from app.api.llm_module import ask_gemini\n",
    "\n",
    "import json\n",
    "\n",
    "\"\"\"\n",
    "    Tạo prompt kết hợp giữa contexts (list record dicts) và câu hỏi.\n",
    "    retrieved_contexts: list of dict (record).\n",
    "    Mỗi record được serialize đầy đủ sang JSON để gửi cho LLM.\n",
    "\"\"\"\n",
    "def build_prompt(user_query: str, retrieved_contexts: List[Dict]) -> str:\n",
    "    context_texts = []\n",
    "    for r in retrieved_contexts:\n",
    "        # Serialize toàn bộ record sang JSON, giữ UTF-8 và readable\n",
    "        record_json = json.dumps(r, ensure_ascii=False, indent=2)\n",
    "        context_texts.append(record_json)\n",
    "\n",
    "    # Nối các record với separator để LLM phân biệt\n",
    "    context_text = \"\\n\\n---\\n\\n\".join(context_texts)\n",
    "\n",
    "    prompt = f\"\"\"\n",
    "Dưới đây là các thông tin tham khảo:\n",
    "\n",
    "{context_text}\n",
    "\n",
    "Câu hỏi của người dùng: {user_query}\n",
    "\n",
    "→ Hãy trả lời BẰNG TIẾNG VIỆT, đầy đủ, súc tích, thân thiện.\n",
    "Nếu không đủ dữ liệu, hãy nói: \"Xin lỗi, tôi chưa có thông tin về điều đó.\"\n",
    "\"\"\"\n",
    "    return prompt.strip()\n",
    "\n",
    "\n",
    "async def generate_answer(user_query: str, retrieved_contexts: List[Dict]) -> str:\n",
    "    try:\n",
    "        prompt = build_prompt(user_query, retrieved_contexts)\n",
    "        return await ask_gemini(prompt)\n",
    "    except Exception as e:\n",
    "        return f\"Lỗi trong generator: {e}\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "553bbb70-1260-4b46-8516-19b31e80840e",
   "metadata": {},
   "source": [
    "# Rag_pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "3b10b5f0-945a-4bef-beb9-405c2fb895ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RAGPipelinePG:\n",
    "    def __init__(\n",
    "        self,\n",
    "        tables: List[str],\n",
    "        splitter_fn=split_documents,\n",
    "        embedder_fn=get_embeddings,\n",
    "        vectorstore_cls=None,\n",
    "        retriever_fn=None,\n",
    "        generator_fn=None,\n",
    "        persist_path=None,\n",
    "        rebuild_on_init=True,\n",
    "        engine=engine\n",
    "    ):\n",
    "        self.tables = tables\n",
    "        self.splitter_fn = splitter_fn\n",
    "        self.embedder_fn = embedder_fn\n",
    "        self.vectorstore_cls = vectorstore_cls or VectorStore\n",
    "        self.retriever_fn = retriever_fn or retrieve_relevant_docs\n",
    "        self.generator_fn = generator_fn or generate_answer\n",
    "        self.persist_path = persist_path\n",
    "        self.rebuild_on_init = rebuild_on_init\n",
    "        self.engine = engine\n",
    "\n",
    "        self._store = None\n",
    "        self._chunks = []\n",
    "        self._vectors = None\n",
    "\n",
    "        self.build_index()\n",
    "\n",
    "    def build_index(self):\n",
    "        \"\"\"\n",
    "        - Nếu đã có index trên disk và rebuild_on_init == False -> load index.\n",
    "        - Ngược lại -> build mới từ loader_fn.\n",
    "        \"\"\"\n",
    "        index_path = None\n",
    "        if self.persist_path:\n",
    "            index_path = os.path.join(self.persist_path, \"faiss.index\")\n",
    "\n",
    "        # Nếu có index trên disk và caller không muốn rebuild -> load\n",
    "        if index_path and os.path.exists(index_path) and not self.rebuild_on_init:\n",
    "            print(\"Loading existing index from persist_path...\")\n",
    "            self._store = self.vectorstore_cls.load(self.persist_path)\n",
    "            return\n",
    "\n",
    "        # Nếu không có index hoặc caller muốn rebuild -> build mới\n",
    "        print(f\"Building new index from tables: {self.tables}\")\n",
    "\n",
    "        data = load_postgres_data_dynamic(self.engine, self.tables)\n",
    "        chunks = self.splitter_fn(data)\n",
    "        self._chunks = chunks\n",
    "\n",
    "        records_for_embed = [{\"record\": c[\"record\"]} for c in chunks]\n",
    "        vectors = self.embedder_fn(records_for_embed)\n",
    "\n",
    "        records = []\n",
    "        for c in chunks:\n",
    "            raw = c[\"record\"]\n",
    "            text = _record_to_text(raw)\n",
    "            records.append({\"raw\": raw, \"text\": text})\n",
    "\n",
    "        self._store = self.vectorstore_cls(\n",
    "            vectors=vectors,\n",
    "            records=records,\n",
    "            persist_path=self.persist_path\n",
    "        )\n",
    "\n",
    "        print(\"Index built and saved.\")\n",
    "\n",
    "\n",
    "    def retrieve_context(self, query, top_k=3):\n",
    "        return self.retriever_fn(query, self._store, top_k=top_k)\n",
    "\n",
    "    async def generate_final_answer(self, question, contexts):\n",
    "        if not contexts:\n",
    "            return \"Xin lỗi, tôi chưa có thông tin về điều đó.\"\n",
    "        return await self.generator_fn(question, contexts)\n",
    "\n",
    "    async def answer(self, question: str, top_k=3):\n",
    "        ctx = self.retrieve_context(question, top_k)\n",
    "        ans = await self.generate_final_answer(question, ctx)\n",
    "        return {\"question\": question, \"answer\": ans, \"contexts\": ctx}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09e17358-ee14-4e2d-b559-3188b6d3a65a",
   "metadata": {},
   "source": [
    "## Test rag_pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "65a1db11-62fe-4a75-b566-87e24b1f16ca",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1. RAG initialized\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|██████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:04<00:00,  4.02s/it]\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'NoneType' object has no attribute 'search'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mAttributeError\u001b[39m                            Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[26]\u001b[39m\u001b[32m, line 13\u001b[39m\n\u001b[32m      1\u001b[39m rag = RAGPipelinePG(\n\u001b[32m      2\u001b[39m     tables=[\n\u001b[32m      3\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mtourism_places\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m      8\u001b[39m     rebuild_on_init=\u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[32m      9\u001b[39m )\n\u001b[32m     11\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33m1. RAG initialized\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m13\u001b[39m result = \u001b[38;5;28;01mawait\u001b[39;00m rag.answer(\u001b[33m\"\u001b[39m\u001b[33mtỉnh : An Giang, rừng\u001b[39m\u001b[33m\"\u001b[39m, top_k=\u001b[32m3\u001b[39m)\n\u001b[32m     15\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m2. KẾT QUẢ TRẢ VỀ:\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     17\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m result:\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[22]\u001b[39m\u001b[32m, line 81\u001b[39m, in \u001b[36mRAGPipelinePG.answer\u001b[39m\u001b[34m(self, question, top_k)\u001b[39m\n\u001b[32m     80\u001b[39m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34manswer\u001b[39m(\u001b[38;5;28mself\u001b[39m, question: \u001b[38;5;28mstr\u001b[39m, top_k=\u001b[32m3\u001b[39m):\n\u001b[32m---> \u001b[39m\u001b[32m81\u001b[39m     ctx = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mretrieve_context\u001b[49m\u001b[43m(\u001b[49m\u001b[43mquestion\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtop_k\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     82\u001b[39m     ans = \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m.generate_final_answer(question, ctx)\n\u001b[32m     83\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m {\u001b[33m\"\u001b[39m\u001b[33mquestion\u001b[39m\u001b[33m\"\u001b[39m: question, \u001b[33m\"\u001b[39m\u001b[33manswer\u001b[39m\u001b[33m\"\u001b[39m: ans, \u001b[33m\"\u001b[39m\u001b[33mcontexts\u001b[39m\u001b[33m\"\u001b[39m: ctx}\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[22]\u001b[39m\u001b[32m, line 73\u001b[39m, in \u001b[36mRAGPipelinePG.retrieve_context\u001b[39m\u001b[34m(self, query, top_k)\u001b[39m\n\u001b[32m     72\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mretrieve_context\u001b[39m(\u001b[38;5;28mself\u001b[39m, query, top_k=\u001b[32m3\u001b[39m):\n\u001b[32m---> \u001b[39m\u001b[32m73\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mretriever_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mquery\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_store\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtop_k\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtop_k\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[20]\u001b[39m\u001b[32m, line 6\u001b[39m, in \u001b[36mretrieve_relevant_docs\u001b[39m\u001b[34m(query, store, top_k, embed_model)\u001b[39m\n\u001b[32m      3\u001b[39m q_vec = get_embeddings([{\u001b[33m\"\u001b[39m\u001b[33mrecord\u001b[39m\u001b[33m\"\u001b[39m: query}])  \u001b[38;5;66;03m# chỉ dùng string, như khi build index\u001b[39;00m\n\u001b[32m      5\u001b[39m \u001b[38;5;66;03m# 2. Lấy top_k records từ vector store\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m6\u001b[39m records = \u001b[43mstore\u001b[49m\u001b[43m.\u001b[49m\u001b[43msearch\u001b[49m(q_vec, top_k=top_k)  \u001b[38;5;66;03m# giả sử trả về list[str] hoặc list[record]\u001b[39;00m\n\u001b[32m      8\u001b[39m \u001b[38;5;66;03m# 3. Map về dict cho RAGPipelinePG\u001b[39;00m\n\u001b[32m      9\u001b[39m contexts = [{\u001b[33m\"\u001b[39m\u001b[33mtitle\u001b[39m\u001b[33m\"\u001b[39m: \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mDoc \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mi\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mbody\u001b[39m\u001b[33m\"\u001b[39m: r} \u001b[38;5;28;01mfor\u001b[39;00m i, r \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(records)]\n",
      "\u001b[31mAttributeError\u001b[39m: 'NoneType' object has no attribute 'search'"
     ]
    }
   ],
   "source": [
    "rag = RAGPipelinePG(\n",
    "    tables=[\n",
    "        \"tourism_places\",\n",
    "        \"vietnam_hotels\",\n",
    "        \"vietnam_foods\"\n",
    "    ],\n",
    "    persist_path=\"../app/rag_store\",\n",
    "    rebuild_on_init=False\n",
    ")\n",
    "\n",
    "print(\"1. RAG initialized\")\n",
    "\n",
    "result = await rag.answer(\"tỉnh : An Giang, rừng\", top_k=3)\n",
    "\n",
    "print(\"\\n2. KẾT QUẢ TRẢ VỀ:\")\n",
    "\n",
    "if not result:\n",
    "    print(\"RAG trả về NONE hoặc RỖNG\")\n",
    "else:\n",
    "    # In câu trả lời\n",
    "    print(\"\\n--- ANSWER ---\")\n",
    "    print(result.get(\"answer\", \"Không có answer\"))\n",
    "\n",
    "    # In các đoạn retrieve\n",
    "    print(\"\\n--- RETRIEVED CONTEXTS ---\")\n",
    "    contexts = result.get(\"contexts\", [])\n",
    "    if len(contexts) == 0:\n",
    "        print(\"Không retrieve được context nào\")\n",
    "    else:\n",
    "        for i, ctx in enumerate(result[\"contexts\"], 1):\n",
    "            print(f\"\\nContext #{i}:\")\n",
    "            print(json.dumps(ctx, ensure_ascii=False, indent=2))\n",
    "\n",
    "\n",
    "print(\"\\n3. DONE\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c62f6505-0934-4b31-a16d-a02eb5557850",
   "metadata": {},
   "source": [
    "## Print result"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
